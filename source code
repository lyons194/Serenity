#NAME: Wishing Well - a web scraping program. 
#LANGUAGE:  Python
#CREATED BY: Sean Lyons
#LAST UPDATED:  September 2, 2019.


#Import modules
from bs4 import BeautifulSoup
import requests
import time
from lxml import html
#Be sure to set up a virtual environment during the set up.  Import virtualenv?


#Functional variables
running = True
chron_memory = []


#stylistic variables

n = '\n'
s = '     '
line = '____________________'
dots = f'.{n}.{n}.{n}.{n}.'

class collect:
    pass


#Internal variables section.

def title():
    time.sleep(1)
    print(f'{n*3}{line*2}{n*2}WISHING WELL -- Main Menu{n*2}{line*2}{n*2}')
    time.sleep(1)

def func():
    #Not yet using 'collect' but may use it in the future.
    collect = []
    var_a = input('Please enter the page URL: ')
    var_b = requests.get(f'{var_a}')
    var_c = html.fromstring(var_b.content)
    var_d = input('Enter HTML variable type: ')
    var_e = input('Enter class name: ')
    var_f = var_c.xpath(f'//{var_d}[@class="{var_e}"]/text()')
    chron_memory.append(f'|URL:  {var_a} |')
    chron_memory.append(f'|HTML:  {var_d} |')
    chron_memory.append(f'|Class:   {var_e} |')
    for item in enumerate(var_f):
        pkg = tuple(item)
        print(pkg)
        chron_memory.append(pkg)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)

def func_timed(x,y):
    #This is the function for the timed scraping selection method as referenced in main().
    starter = 0
    ft_frame = []
    while starter <= y:
        clock = time.sleep(x)
        print('Scheduled mining in progress.  To cancel, press ctrl + c.')
        print(f'Loading...')
    ft_collect = []
    ft_var_a = input('Please enter the page URL: ')
    ft_var_b = requests.get(f'{var_a}')
    ft_var_c = html.fromstring(var_b.content)
    ft_var_d = input('Enter HTML variable type: ')
    ft_var_e = input('Enter class name: ')
    ft_var_f = ft_var_c.xpath(f'//{ft_var_d}[@class="{ft_var_e}"]/text()')
    chron_memory.append(f'|URL:  {ft_var_a} |')
    chron_memory.append(f'|HTML:  {ft_var_d} |')
    chron_memory.append(f'|Class:   {ft_var_e} |')
    for item in enumerate(var_f):
        pkg = tuple(item)
        print(pkg)
        chron_memory.append(pkg)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)


#User-interactive variables section.

#Bank function prototype.  To be defined.  Used to view stored data.
def bank():
    #Serial_collect is a list that collects all data in a serialized manner, regardless of batch.
    serial_collect = []
    #Batch_collect compiles each scraping session as a batch and stores that data in dictionary form.
    batch_collect = {}
    print(f'{n*3}{line*2}{n*2}YOUR STORED DATA:{n*2}{line*2}{n*3}')
    for item in enumerate(chron_memory):
        print(item)


#Help function prototype.  To be defined.  Used to access help menu.
def help_menu():
    help_list = ['bank', 'exit', 'export', 'help', 'scrape']
    print(f'{n*3}{line*2}{n*2}HELP MENU:{n*2}{line*2}{n*3}')
    print(f'{n}Please select one of the following words:{n}')
    for item in enumerate(help_list):
        print(f'{n}{item}')
    help_select = input(f'{n*2}Please enter the keyword you need help with:{n*2}Keyword = ')
    if help_select == 'bank':
        print(f'{n}The "bank" function displays the data the user has scraped within a single session.{n}')
        cont = input(f'{n*2}Continue? ')
        if cont == 'y':
            pass
    elif help_select == 'exit':
        print(f'{n}The "exit" command allows the user to exit the program.{n}')
        cont = input(f'{n*2}Continue? ')
        if cont == 'y':
            pass
    elif help_select == 'export':
        print(f'{n}The "export" command allows the user to export their collected data.{n}')
        cont = input(f'{n*2}Continue? ')
        if cont == 'y':
            pass
    elif help_select == 'help':
        print(f'{n}The "help" command allows the user to access the help menu of this program.{n}')
        cont = input(f'{n*2}Continue? ')
        if cont == 'y':
            pass
    elif help_select == 'scrape':
        print(f'{n}The "scrape" command allows the user to begin a new web scraping session.{n}')
        cont = input(f'{n*2}Continue? ')
        if cont == 'y':
            pass
        
        
#Export function prototype.  To be defined.
def export():
    print(f'{n*3}{line*2}{n*2}EXPORT MENU:{n*2}{line*2}{n*3}')


#Defining the main() function.
def main():
    while running:
        time.sleep(1)
        menu = input(f'{n*3}**For help, enter "help".{n*3}Please enter a command:  ')
        if menu == 'scrape':
            time.sleep(1)
            print(f'{n*3}{line*2}{n*2}WISHING WELL -- Web Scraping Menu{n*2}{line*2}{n*2}')
            print(f'{n*3}Select which method to use:{n}')
            print(f'{n}* One-time web scraping session. -- Press [1] + [Enter]{n}')
            print(f'{n}* Reoccurring web scraping session. -- Press [2] + [Enter]{n*2}')
            scrape_answer = input('Enter method number: ')
            if scrape_answer == '1':
                #Hippo.  Change the variable names from this point downward until 'continue here'.
                print(f'{n*3}{line*2}{n*2}WISHING WELL -- One-Time Scraping Session{n*2}{line*2}{n*2}')
                OT_start = input(f'{n*2}Please enter your full URL below.{n*2}URL: ')
                print(f'{n*2}URL Checker:{n*2}{OT_start}{n}')
                OT_check = input(f'{n*2}Is this URL accurate?{n*2}(y/n): ')
                if OT_check == 'y':
                    OT_re = requests.get(f'{OT_start}/robots.txt')
                    OT_data = OT_re.text
                    OT_soup = BeautifulSoup(OT_data, features='lxml')
                    OT_result = OT_soup.find_all
                    print(f'{n*5}ROBOTS.TXT PRE-CHECK.{n}{line}{n*5}{OT_result}{n*2}{line}{n*2}')
                    OT_con = input('Continue? ')
                    if OT_con == 'y':
                        time.sleep(1)
                        print(f'{n*2}OK!{n*2}')
                        func()
                    elif OT_con == 'n':
                        pass
            elif scrape_answer == '2':
                #Continue here.  Don't forget to change the variable names up until (hashtag)hippo.
                print(f'{n*3}{line*2}{n*2}WISHING WELL -- Reoccurring Scraping Session{n*2}{line*2}{n*2}')
                R_start = input(f'{n*2}Please enter your full URL below.{n*2}URL: ')
                print(f'{n*2}URL Checker:{n*2}{R_start}{n}')
                R_check = input(f'{n*2}Is this URL accurate?{n*2}(y/n): ')
                if R_check == 'y':
                    R_re = requests.get(f'{R_start}/robots.txt')
                    R_data = R_re.text
                    R_soup = BeautifulSoup(R_data, features='lxml')
                    R_result = R_soup.find_all
                    print(f'{n*5}ROBOTS.TXT PRE-CHECK.{n}{line}{n*5}{R_result}{n*2}{line}{n*2}')
                    R_con = input('Continue? ')
                    if R_con == 'y':
                        time.sleep(1)
                        print(f'{n*2}OK!')
                        timed_func()
                    elif R_con == 'n':
                        pass
                elif check == 'n':
                    pass
        elif menu == 'export':
            export()
        elif menu == 'help':
            help_menu()
        elif menu == 'bank':
            bank()
        elif menu == 'exit':
            quit()

#Thought:  use the 'try' function for the robots.txt request function.

if __name__ == '__main__':
    title()
    main()



#Import modules

from bs4 import BeautifulSoup
import requests
import time
from lxml import html
#Be sure to set up a virtual environment during the set up.  Import virtualenv?
#
#Functional variables
running = True
chron_memory = []
#
#stylistic variables
#
n = '\n'
s = '     '
line = '____________________'
dots = f'.{n}.{n}.{n}.{n}.'
#
class collect:
    pass
#
#Internal variables section.
#
def title():
    time.sleep(1)
    print(f'{n*3}{line*2}{n*2}WISHING WELL -- Main Menu{n*2}{line*2}{n*2}')
    time.sleep(1)
#
def func():
    #
    #Begin variable definitions.
    #
    var_a = input('Please enter the page URL: ')
    var_b = requests.get(f'{var_a}')
    var_c = html.fromstring(var_b.content)
    var_d = input('Enter HTML variable type: ')
    var_e = input('Enter class name: ')
    var_f = var_c.xpath(f'//{var_d}[@class="{var_e}"]/text()')
    chron_memory.append(f'|URL:  {var_a} |')
    chron_memory.append(f'|HTML:  {var_d} |')
    chron_memory.append(f'|Class:   {var_e} |')
    #
    #End variable definitions.
    #
    #
    #Begin organizational functions.
    #
    for item in enumerate(var_f):
        pkg = tuple(item)
        print(pkg)
        chron_memory.append(pkg)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)
    #
    #End organizational functions.
    #

def func_timed(x,y):
    #This is the function for the timed scraping selection method as referenced in main().
    starter = 0
    ft_frame = []
    while starter <= y:
        clock = time.sleep(x)
        print('Scheduled mining in progress.  To cancel, press ctrl + c.')
        print(f'Loading...')
    ft_collect = []
    ft_var_a = input('Please enter the page URL: ')
    ft_var_b = requests.get(f'{var_a}')
    ft_var_c = html.fromstring(var_b.content)
    ft_var_d = input('Enter HTML variable type: ')
    ft_var_e = input('Enter class name: ')
    ft_var_f = ft_var_c.xpath(f'//{ft_var_d}[@class="{ft_var_e}"]/text()')
    chron_memory.append(f'|URL:  {ft_var_a} |')
    chron_memory.append(f'|HTML:  {ft_var_d} |')
    chron_memory.append(f'|Class:   {ft_var_e} |')
    for item in enumerate(var_f):
        pkg = tuple(item)
        print(pkg)
        chron_memory.append(pkg)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)
    chron_memory.append(collect)
#
#User-interactive variables section.
#
#
#Bank function prototype.  To be defined.  Used to view stored data.
#
def bank():
    #Serial_collect is a list that collects all data in a serialized manner, regardless of batch.
    serial_collect = []
    #Batch_collect compiles each scraping session as a batch and stores that data in dictionary form.
    batch_collect = {}
    print(f'{n*3}{line*2}{n*2}YOUR STORED DATA:{n*2}{line*2}{n*3}')
    for item in enumerate(chron_memory):
        print(item)
#
#Help function prototype.  To be defined.  Used to access help menu.
#
def help_menu():
    #
    #Begin functional variables and title.
    #
    help_list = ['bank', 'exit', 'export', 'help', 'scrape']
    print(f'{n*3}{line*2}{n*2}HELP MENU:{n*2}{line*2}{n*3}')
    print(f'{n}Please select one of the following words:{n}')
    #
    #End of functional variables and title.
    #
    #
    #Begin hrlp menu keywords menu.
    #
    for item in enumerate(help_list):
        print(f'{n}{item}')
    help_select = input(f'{n*2}Please enter the keyword you need help with:{n*2}Keyword = ')
    if help_select == 'bank':
        print(f'{n}The "bank" function displays the data the user has scraped within a single session.{n}')
        cont = input(f'{n*2}Continue? ')
        if cont == 'y':
            pass
    elif help_select == 'exit':
        print(f'{n}The "exit" command allows the user to exit the program.{n}')
        cont = input(f'{n*2}Continue? ')
        if cont == 'y':
            pass
    elif help_select == 'export':
        print(f'{n}The "export" command allows the user to export their collected data.{n}')
        cont = input(f'{n*2}Continue? ')
        if cont == 'y':
            pass
    elif help_select == 'help':
        print(f'{n}The "help" command allows the user to access the help menu of this program.{n}')
        cont = input(f'{n*2}Continue? ')
        if cont == 'y':
            pass
    elif help_select == 'scrape':
        print(f'{n}The "scrape" command allows the user to begin a new web scraping session.{n}')
        cont = input(f'{n*2}Continue? ')
        if cont == 'y':
            pass
        #
        #End of help menu keywords menu.
        #
#       
#Export function prototype.  To be defined.
#
def export():
    print(f'{n*3}{line*2}{n*2}EXPORT MENU:{n*2}{line*2}{n*3}')
#
#Defining the main() function.
#
def main():
    while running:
        time.sleep(1)
        menu = input(f'{n*3}**For help, enter "help".{n*3}Please enter a command:  ')
        #
        #Beginning of scrape menu option.
        #
        if menu == 'scrape':
            time.sleep(1)
            print(f'{n*3}{line*2}{n*2}WISHING WELL -- Web Scraping Menu{n*2}{line*2}{n*2}')
            print(f'{n*3}Select which method to use:{n}')
            print(f'{n}* One-time web scraping session. -- Press [1] + [Enter]{n}')
            print(f'{n}* Reoccurring web scraping session. -- Press [2] + [Enter]{n*2}')
            scrape_answer = input('Enter method number: ')
            if scrape_answer == '1':
                #
                #Beginning of singular web scraping session section.
                #
                print(f'{n*3}{line*2}{n*2}WISHING WELL -- One-Time Scraping Session{n*2}{line*2}{n*2}')
                OT_start = input(f'{n*2}Please enter your full URL below.{n*2}URL: ')
                print(f'{n*2}URL Checker:{n*2}{OT_start}{n}')
                OT_check = input(f'{n*2}Is this URL accurate?{n*2}(y/n): ')
                if OT_check == 'y':
                    OT_re = requests.get(f'{OT_start}/robots.txt')
                    OT_data = OT_re.text
                    OT_soup = BeautifulSoup(OT_data, features='lxml')
                    OT_result = OT_soup.find_all
                    print(f'{n*5}ROBOTS.TXT PRE-CHECK.{n}{line}{n*5}{OT_result}{n*2}{line}{n*2}')
                    OT_con = input('Continue? ')
                    if OT_con == 'y':
                        time.sleep(1)
                        print(f'{n*2}OK!{n*2}')
                        func()
                    elif OT_con == 'n':
                        pass
                    #
                    #End of singular web scraping session section.
                    #
            elif scrape_answer == '2':
                #
                #Beginning of reoccurring web scraping session section.
                #
                print(f'{n*3}{line*2}{n*2}WISHING WELL -- Reoccurring Scraping Session{n*2}{line*2}{n*2}')
                R_start = input(f'{n*2}Please enter your full URL below.{n*2}URL: ')
                print(f'{n*2}URL Checker:{n*2}{R_start}{n}')
                R_check = input(f'{n*2}Is this URL accurate?{n*2}(y/n): ')
                if R_check == 'y':
                    R_re = requests.get(f'{R_start}/robots.txt')
                    R_data = R_re.text
                    R_soup = BeautifulSoup(R_data, features='lxml')
                    R_result = R_soup.find_all
                    print(f'{n*5}ROBOTS.TXT PRE-CHECK.{n}{line}{n*5}{R_result}{n*2}{line}{n*2}')
                    R_con = input('Continue? ')
                    if R_con == 'y':
                        time.sleep(1)
                        print(f'{n*2}OK!')
                        timed_func_one = int(input('Please enter your scraping time interval: '))
                        timed_func_two = int(input('Please enter the number of scraping sessions: '))
                        timed_func(timed_func_one, timed_func_two)
                    elif R_con == 'n':
                        pass
                    #
                    #End of reoccurring web scraping session section.
                    #
                elif check == 'n':
                    pass
        #
        #End of scrape menu option.
        #
        elif menu == 'export':
            export()
        elif menu == 'help':
            help_menu()
        elif menu == 'bank':
            bank()
        elif menu == 'exit':
            quit()

#Thought:  use the 'try' function for the robots.txt request function.

if __name__ == '__main__':
    title()
    main()
